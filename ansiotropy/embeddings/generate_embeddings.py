### Extract contextual embeddings from transformer models

# input: corpus with words in context
# output: dictionary with word in corpus
# also which layer are we using
# other considerations: model architecture
# ideally supported model architectures
# BERT, roBERTa (DART), T5, GPT, GPT2, XLM, BERT whitening


# logic for recovering soft prompt tokens

# Ansiotropy Metrics
# 1. Self Similarity
# 2. Intra Sentence Similarity
# 3. Maximum explainable variance:


# 1 and 3 can be computed post hoc from matrices
# 2 is useful to calculate while iterateting over sentences
